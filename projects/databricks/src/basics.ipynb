{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('spork').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+----+-------+\n",
      "|age |name   |\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|30  |Andy   |\n",
      "|19  |Justin |\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('../data/people.json')\n",
    "df.printSchema()\n",
    "\n",
    "# has two potential parameter\n",
    "df.show(n=3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter based on the col operation startswith\n",
    "df.filter(df['name'].startswith('M')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# limit returns a limited subset of the rows\n",
    "df.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct() alias for dropDuplicates()\n",
    "# cache() alias for persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caches data for quick recall\n",
    "df.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = ( \n",
    "    spark.read\n",
    "         .option('header', 'true')\n",
    "         .option('inferSchema', 'true')\n",
    "         .csv('../data/walmart_stock.csv') \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df2.limit(5)\n",
    "       .select('Date', 'Open')\n",
    "       .write\n",
    "       .parquet('../data/goose.parquet')\n",
    "#        .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basics.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-4d84c95e-b776-4e75-a901-8b9e117a18b4-c000.snappy.parquet  _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "ls ../data/goose.parquet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|               Date|              Open|\n",
      "+-------------------+------------------+\n",
      "|2012-01-03 00:00:00|         59.970001|\n",
      "|2012-01-04 00:00:00|60.209998999999996|\n",
      "|2012-01-05 00:00:00|         59.349998|\n",
      "|2012-01-06 00:00:00|         59.419998|\n",
      "|2012-01-09 00:00:00|         59.029999|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = (spark.read\n",
    "            .parquet('../data/goose.parquet')\n",
    "      )\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# Replace FILL_IN with your code. You will probably need multiple\n",
    "# lines of code for this problem.\n",
    "\n",
    "parquetDir = \"/mnt/training/wikipedia/pagecounts/staging_parquet_en_only_clean/\"\n",
    "\n",
    "washingtons = spark.read.parquet(parquetDir)\n",
    "washingtons = (washingtons.filter(washingtons['project'] == 'en')\n",
    "                          .filter(washingtons['article'].endswith('_Washington'))\n",
    "                          .collect())\n",
    "totalWashingtons = 0\n",
    "\n",
    "for washington in washingtons:\n",
    "  totalWashingtons += washington['requests']\n",
    "  \n",
    "print(\"Total Washingtons: {0:,}\".format( len(washingtons) ))\n",
    "print(\"Total Washington Requests: {0:,}\".format( totalWashingtons ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "parquetDir = \"/mnt/training/wikipedia/pagecounts/staging_parquet_en_only_clean/\"\n",
    "\n",
    "washingtons = (spark.read\n",
    "                    .parquet(parquetDir)\n",
    "                    .filter(\"project == 'en'\")\n",
    "                    .filter(col('article').endswith('_Washington'))\n",
    "                    .collect())\n",
    "\n",
    "totalWashingtons = 0\n",
    "\n",
    "for washington in washingtons:\n",
    "  totalWashingtons += washington['requests']\n",
    "  \n",
    "print(\"Total Washingtons: {0:,}\".format( len(washingtons) ))\n",
    "print(\"Total Washington Requests: {0:,}\".format( totalWashingtons ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempA = (initialDF\n",
    "  .withColumnRenamed(\"timestamp\", \"capturedAt\")\n",
    "  .select( col(\"*\"), unix_timestamp( col(\"capturedAt\"), \"yyyy-MM-dd HH:mm:ss\") )\n",
    ")\n",
    "tempA.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columnC: Column<b'requests'>\n",
      "columnD: Column<b'(a + 1)'>\n",
      "columnE: Column<b'abc'>\n"
     ]
    }
   ],
   "source": [
    "# If we import ...sql.functions, we get a couple of more options:\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# This uses the col(..) function\n",
    "columnC = col(\"requests\")\n",
    "\n",
    "# This uses the expr(..) function which parses an SQL Expression\n",
    "columnD = expr(\"a + 1\")\n",
    "\n",
    "# This uses the lit(..) to create a literal (constant) value.\n",
    "columnE = lit(\"abc\")\n",
    "\n",
    "# Print the type of each attribute\n",
    "print(\"columnC: {}\".format(columnC))\n",
    "print(\"columnD: {}\".format(columnD))\n",
    "print(\"columnE: {}\".format(columnE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+--------+\n",
      "|Person |Sales|New Sales|Constant|\n",
      "+-------+-----+---------+--------+\n",
      "|Sam    |200.0|201.0    |2       |\n",
      "|Charlie|120.0|121.0    |2       |\n",
      "|Frank  |340.0|341.0    |2       |\n",
      "|Tina   |600.0|601.0    |2       |\n",
      "|Amy    |124.0|125.0    |2       |\n",
      "|Vanessa|243.0|244.0    |2       |\n",
      "|Carl   |870.0|871.0    |2       |\n",
      "|Sarah  |350.0|351.0    |2       |\n",
      "|John   |250.0|251.0    |2       |\n",
      "|Linda  |130.0|131.0    |2       |\n",
      "+-------+-----+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr, lit\n",
    "\n",
    "dataframe = (spark.read\n",
    "                  .option('header', 'true')\n",
    "                  .option('inferSchema', 'true')\n",
    "                  .csv('../data/sales_info.csv')\n",
    "            )\n",
    "\n",
    "dataframe = dataframe.select(col('Person'), col('Sales'))\n",
    "dataframe = dataframe.withColumn('New Sales', expr('Sales + 1'))\n",
    "dataframe = dataframe.withColumn('Constant', lit('2'))\n",
    "\n",
    "dataframe.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLLECT:\n",
      "Row(Person='Sam', Sales=200.0, New Sales=201.0, Constant='2')\n",
      "Row(Person='Charlie', Sales=120.0, New Sales=121.0, Constant='2')\n",
      "Row(Person='Frank', Sales=340.0, New Sales=341.0, Constant='2')\n",
      "Row(Person='Tina', Sales=600.0, New Sales=601.0, Constant='2')\n",
      "Row(Person='Amy', Sales=124.0, New Sales=125.0, Constant='2')\n",
      "Row(Person='Vanessa', Sales=243.0, New Sales=244.0, Constant='2')\n",
      "Row(Person='Carl', Sales=870.0, New Sales=871.0, Constant='2')\n",
      "Row(Person='Sarah', Sales=350.0, New Sales=351.0, Constant='2')\n",
      "Row(Person='John', Sales=250.0, New Sales=251.0, Constant='2')\n",
      "Row(Person='Linda', Sales=130.0, New Sales=131.0, Constant='2')\n",
      "Row(Person='Mike', Sales=750.0, New Sales=751.0, Constant='2')\n",
      "Row(Person=' Chris', Sales=350.0, New Sales=351.0, Constant='2')\n",
      "\n",
      "TAKE 3:\n",
      "Row(Person='Sam', Sales=200.0, New Sales=201.0, Constant='2')\n",
      "Row(Person='Charlie', Sales=120.0, New Sales=121.0, Constant='2')\n",
      "Row(Person='Frank', Sales=340.0, New Sales=341.0, Constant='2')\n"
     ]
    }
   ],
   "source": [
    "print('COLLECT:')\n",
    "for row in dataframe.collect():\n",
    "    print(row)\n",
    "    \n",
    "print('\\nTAKE 3:')\n",
    "for row in dataframe.take(3):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+--------+\n",
      "| Person|Sales|New Sales|Constant|\n",
      "+-------+-----+---------+--------+\n",
      "|    Sam|200.0|    201.0|       2|\n",
      "|Charlie|120.0|    121.0|       2|\n",
      "|  Frank|340.0|    341.0|       2|\n",
      "|   Tina|600.0|    601.0|       2|\n",
      "|    Amy|124.0|    125.0|       2|\n",
      "|Vanessa|243.0|    244.0|       2|\n",
      "|   Carl|870.0|    871.0|       2|\n",
      "|  Sarah|350.0|    351.0|       2|\n",
      "|   John|250.0|    251.0|       2|\n",
      "|  Linda|130.0|    131.0|       2|\n",
      "|   Mike|750.0|    751.0|       2|\n",
      "|  Chris|350.0|    351.0|       2|\n",
      "+-------+-----+---------+--------+\n",
      "\n",
      "+------+\n",
      "|   col|\n",
      "+------+\n",
      "|   Sam|\n",
      "|Charli|\n",
      "|      |\n",
      "| Frank|\n",
      "|  Tina|\n",
      "|   Amy|\n",
      "|   Van|\n",
      "|   ssa|\n",
      "|  Carl|\n",
      "| Sarah|\n",
      "|  John|\n",
      "| Linda|\n",
      "|   Mik|\n",
      "|      |\n",
      "| Chris|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.show()\n",
    "dataframe.select(explode(split(col('Person'), 'e'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|      g|\n",
      "+-------+\n",
      "|    Sam|\n",
      "|Charlie|\n",
      "|  Frank|\n",
      "|   Tina|\n",
      "|    Amy|\n",
      "|Vanessa|\n",
      "|   Carl|\n",
      "|  Sarah|\n",
      "|   John|\n",
      "|  Linda|\n",
      "|   Mike|\n",
      "|  Chris|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.select(col('Person').alias('g')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'userhome' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-5902ef2bc316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msourceFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"dbfs:/mnt/training/dataframes/people-with-dups.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdestFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserhome\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/people.parquet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'userhome' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "sourceFile = \"dbfs:/mnt/training/dataframes/people-with-dups.txt\"\n",
    "destFile = userhome + \"/people.parquet\"\n",
    "\n",
    "\n",
    "# In case it already exists\n",
    "dbutils.fs.rm(destFile, True)\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)\n",
    "\n",
    "(spark.read\n",
    "      .option('sep', ':')\n",
    "      .option('header', 'true')\n",
    "      .csv(sourceFile)\n",
    "      .select(initcap(lower(col('firstName'))).alias('firstName'), initcap(lower(col('middleName'))).alias('middleName'), initcap(lower(col('lastName'))).alias('lastName'), 'gender', 'birthDate', 'salary', regexp_replace(col('ssn'), r'(-)', '').alias('ssn'))\n",
    "      .orderBy('firstName', 'lastName')\n",
    "      .dropDuplicates()\n",
    "      .write\n",
    "      .parquet(destFile)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'userhome' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-ab905b483c61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msourceFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"dbfs:/mnt/training/dataframes/people-with-dups.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdestFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserhome\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/people.parquet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'userhome' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "sourceFile = \"dbfs:/mnt/training/dataframes/people-with-dups.txt\"\n",
    "destFile = userhome + \"/people.parquet\"\n",
    "\n",
    "\n",
    "# In case it already exists\n",
    "dbutils.fs.rm(destFile, True)\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)\n",
    "\n",
    "(spark.read\n",
    "      .option('sep', ':')\n",
    "      .option('header', 'true')\n",
    "      .csv(sourceFile)\n",
    "      .withColumn('lcFirst', lower(col('firstName')))\n",
    "      .withColumn('lcMiddle', lower(col('middleName')))\n",
    "      .withColumn('lcLast', lower(col('lastName')))\n",
    "      .withColumn('fixSsn', regexp_replace(col('ssn'), r'(-)', ''))\n",
    "      .dropDuplicates(['lcFirst', 'lcMiddle', 'lcLast', 'gender', 'salary', 'birthDate', 'fixSsn'])\n",
    "      .drop('lcFirst', 'lcMiddle', 'lcLast', 'fixSsn')\n",
    "      .write\n",
    "      .parquet(destFile)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
