{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|100| aaa|\n",
      "|120| bbb|\n",
      "|150| ccc|\n",
      "|200| aaa|\n",
      "|220| aaa|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('abc').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "tmp_data = [(100,\"aaa\"),(120,\"bbb\"),(150,\"ccc\"),(200,\"aaa\"),(220,\"aaa\")]\n",
    "tmp_row = Row(\"id\",\"name\")\n",
    "rdd_row = sc.parallelize(tmp_data)\n",
    "rdd_schema = rdd_row.map(lambda x: tmp_row(*x))\n",
    "\n",
    "\n",
    "df_employee = spark.createDataFrame(rdd_schema)\n",
    "df_employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|100| aaa|\n",
      "|200| aaa|\n",
      "|220| aaa|\n",
      "+---+----+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(name#1) && (name#1 = aaa))\n",
      "+- Scan ExistingRDD[id#0L,name#1]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_employee.filter(col(\"name\") == \"aaa\").show()\n",
    "df_employee.filter(col(\"name\") == \"aaa\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('name = aaa)\n",
      "+- LogicalRDD [id#0L, name#1], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, name: string\n",
      "Filter (name#1 = aaa)\n",
      "+- LogicalRDD [id#0L, name#1], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(name#1) && (name#1 = aaa))\n",
      "+- LogicalRDD [id#0L, name#1], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(name#1) && (name#1 = aaa))\n",
      "+- Scan ExistingRDD[id#0L,name#1]\n"
     ]
    }
   ],
   "source": [
    "df_employee.filter(col(\"name\") == \"aaa\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employee.createOrReplaceTempView(\"vw_employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|100| aaa|\n",
      "|200| aaa|\n",
      "|220| aaa|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from vw_employee where name = 'aaa'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [*]\n",
      "+- 'Filter ('name = aaa)\n",
      "   +- 'UnresolvedRelation `vw_employee`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, name: string\n",
      "Project [id#0L, name#1]\n",
      "+- Filter (name#1 = aaa)\n",
      "   +- SubqueryAlias `vw_employee`\n",
      "      +- LogicalRDD [id#0L, name#1], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(name#1) && (name#1 = aaa))\n",
      "+- LogicalRDD [id#0L, name#1], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(name#1) && (name#1 = aaa))\n",
      "+- Scan ExistingRDD[id#0L,name#1]\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from vw_employee where name = 'aaa'\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|dept_name|\n",
      "+---+---------+\n",
      "|100|  finance|\n",
      "|120|marketing|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp_list_dept = [(100,\"finance\"),(120,\"marketing\")]\n",
    "top_row_dept = Row(\"id\",\"dept_name\")\n",
    "tmp_rdd_dept = sc.parallelize(tmp_list_dept)\n",
    "tmp_rdd_dept_schema = tmp_rdd_dept.map(lambda x: top_row_dept(*x)) \n",
    "df_dept = spark.createDataFrame(tmp_rdd_dept_schema)\n",
    "df_dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employee.join(df_dept, df_dept.id == df_employee.id,\"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employee.join(df_dept, df_dept.id == df_employee.id,\"inner\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dept.createOrReplaceTempView(\"vw_dept\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from vw_employee join vw_dept on vw_employee.id = vw_dept.id \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from vw_employee join vw_dept on vw_employee.id = vw_dept.id \").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employee.join(df_dept.hint(\"broadcast\"),\"ID\",\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employee.join(df_dept,\"id\",\"left\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employee.join(df_dept.hint(\"broadcast\"),\"ID\",\"left\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "tmp_data = [(100,\"aaa\"),(120,\"bbb\"),(150,\"ccc\"),(200,\"aaa\"),(220,\"aaa\")]\n",
    "tmp_row = Row(\"id\",\"name\")\n",
    "rdd_row = sc.parallelize(tmp_data)\n",
    "rdd_schema = rdd_row.map(lambda x: tmp_row(*x))\n",
    "\n",
    "df_employee = spark.createDataFrame(rdd_schema)\n",
    "df_employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by\n",
    "df_employee.groupBy(\"name\").count().sort(\"count\").show()\n",
    "df_employee.groupBy(\"name\").count().sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "tmp_data = [(100,\"aaa\",5000),(120,\"bbb\",2000),(150,\"ccc\",5000),(200,\"aaa\",1000),(220,\"aaa\",3000)]\n",
    "tmp_row = Row(\"id\",\"name\",\"salary\")\n",
    "rdd_row = sc.parallelize(tmp_data)\n",
    "rdd_schema = rdd_row.map(lambda x: tmp_row(*x))\n",
    "\n",
    "df_employee = spark.createDataFrame(rdd_schema)\n",
    "df_employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employee.groupBy(\"name\").sum(\"salary\").show()\n",
    "df_employee.groupBy(\"name\").sum(\"salary\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Catalyst supports both rule-based and cost-based optimization.\n",
    "1. cost based Optimizer(CBO): If a sql query can be executed in 2 different ways ( like may have path 1 and path2 for same query),then What CBO does is, it basically calculates the cost of each path and the analyses for which path the cost of execution is less and then executes that path so that it can optimize the quey execution.\n",
    "\n",
    "2. Rule base optimizer(RBO): this basically follows the rules which are needed for executing a query. So depending on the number of rules which are to be applied, the optimzer runs the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employee.select(\"name\",\"id\").explain()\n",
    "df_employee.select(\"name\",\"id\").limit(2).explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "name": "spark_explain_plan",
  "notebookId": 441836281736275
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
